{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1lvx+QcdQcxlATH30TOT5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffyelson/IntroductiontoDeepLearning/blob/main/IDLAssignment_01_MNIST_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing TensorFlow, Numpy , Matplotlib and datasets.py\n"
      ],
      "metadata": {
        "id": "LF28bdoVnVZV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bZ4jwWGYe_jC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import MNISTDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading the MNIST Dataset"
      ],
      "metadata": {
        "id": "4w8Dgq22oBJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "id": "6gKVvE8NpOkT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(train_images[10], cmap=\"Greys_r\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "7WT5GZ2TpmHC",
        "outputId": "02deddf4-43fd-4a19-9175-9320034f4c45"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1e3f134d90>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[10].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTQItVNYp_1T",
        "outputId": "d16e7e6b-53bc-4873-8b8d-c5cf7f06bae4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Reshaping the 28*28 image pixels to a column"
      ],
      "metadata": {
        "id": "TqzOiVf3rEAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = MNISTDataset(train_images.reshape([-1, 784]), train_labels, \n",
        "                    test_images.reshape([-1, 784]), test_labels,\n",
        "                    batch_size=128)"
      ],
      "metadata": {
        "id": "mx6CJgBLrlph"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Shape and Size of Entire Dataset\n",
        "data.train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgkiKAOGru-R",
        "outputId": "32977abb-d798-4fbe-8a1f-9a222e407b12"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Reshaped Training Data\n",
        "data.train_data[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg6f_vDMsKPy",
        "outputId": "6e211cd2-0367-449b-c31c-01c2e6c633fe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(784,)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initialization of weights and biases"
      ],
      "metadata": {
        "id": "4JsDqhX3tefb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we initialize all the weights to zero then during backpropagation the gradient calculated will be same for all the hidden layers. Therefore this will lead to a deterministic output and all the hidden layer units will be symmetric. We need to therefore do a symmetry breaking initialization. We can randomly initialize values. Two standard distributions to initialize the network parameters are the Gaussian or uniform distributions.\n",
        "\n",
        "Random Normal Distribution in TensorFlow\n",
        "tf.random.normal(\n",
        "    shape,\n",
        "    mean=0.0,\n",
        "    stddev=1.0,\n",
        "    dtype=tf.dtypes.float32,\n",
        "    seed=None,\n",
        "    name=None\n",
        ")\n",
        "\n",
        "\n",
        "*   The initialization has usually zero mean and standard deviation one.\n",
        "\n",
        "*   The biases are usually set to zeros or ones."
      ],
      "metadata": {
        "id": "yuHHeC5otpBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "######Initialization-Normal + ActivationFunction=ReLu"
      ],
      "metadata": {
        "id": "yq22_zdHspFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = tf.Variable(tf.random.normal(shape = [784, 128],\n",
        "    mean=0.0, stddev=0.05, seed=None))\n",
        "b1 = tf.Variable(np.zeros(128, dtype=np.float32))\n",
        "\n",
        "W2 = tf.Variable(tf.random.normal(shape = [128, 10],\n",
        "    mean=0.0, stddev=0.05, seed=None))\n",
        "b2 = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "\n",
        "def model1(inputs):\n",
        "  #Computing the hidden layers - We choose relu activation function\n",
        "  h1 = tf.nn.relu(tf.matmul(inputs,W1)+b1)\n",
        "  h2 = tf.nn.relu(tf.matmul(h1,W2)+b2)\n",
        "\n",
        "  #I tried sigmoid activation function but it has around 83 - 85 percent accuracy. ReLu performs better.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return h2\n",
        "\n"
      ],
      "metadata": {
        "id": "I8nkCy5MyQv6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_steps = 2000\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "jGTECQuz9nTx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "for step in range(train_steps+1):\n",
        "    image_batch, label_batch = data.next_batch()\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        logits = model1(image_batch)\n",
        "        xent = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=label_batch))\n",
        "  \n",
        "    grads = tape.gradient(xent, [W1, b1])\n",
        "    W1.assign_sub(learning_rate * grads[0])\n",
        "    b1.assign_sub(learning_rate * grads[1])\n",
        "\n",
        "    grads2 = tape.gradient(xent, [W2, b2])\n",
        "    W2.assign_sub(learning_rate * grads2[0])\n",
        "    b2.assign_sub(learning_rate * grads2[1])\n",
        "\n",
        "    \n",
        "    # every so often we print loss/accuracy\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch),\n",
        "                             tf.float32))\n",
        "        print(\"Step {}. Batch loss: {} Batch accuracy: {}\".format(step+1, xent, acc))\n",
        "    \n",
        "del tape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0wWUnPZ_Lg9",
        "outputId": "e48c978e-c2fa-42b3-98b7-986407b81dab"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1. Batch loss: 2.3046133518218994 Batch accuracy: 0.109375\n",
            "Step 101. Batch loss: 0.5693728923797607 Batch accuracy: 0.8671875\n",
            "Step 201. Batch loss: 0.45631730556488037 Batch accuracy: 0.8671875\n",
            "Step 301. Batch loss: 0.38184309005737305 Batch accuracy: 0.9140625\n",
            "Step 401. Batch loss: 0.3502623438835144 Batch accuracy: 0.8984375\n",
            "Starting new epoch...\n",
            "Step 501. Batch loss: 0.3632490336894989 Batch accuracy: 0.9140625\n",
            "Step 601. Batch loss: 0.17942240834236145 Batch accuracy: 0.953125\n",
            "Step 701. Batch loss: 0.27478712797164917 Batch accuracy: 0.90625\n",
            "Step 801. Batch loss: 0.3601808249950409 Batch accuracy: 0.875\n",
            "Step 901. Batch loss: 0.30228471755981445 Batch accuracy: 0.890625\n",
            "Starting new epoch...\n",
            "Step 1001. Batch loss: 0.15435072779655457 Batch accuracy: 0.96875\n",
            "Step 1101. Batch loss: 0.19857802987098694 Batch accuracy: 0.953125\n",
            "Step 1201. Batch loss: 0.130287766456604 Batch accuracy: 0.9609375\n",
            "Step 1301. Batch loss: 0.25763261318206787 Batch accuracy: 0.9140625\n",
            "Step 1401. Batch loss: 0.17950233817100525 Batch accuracy: 0.9296875\n",
            "Starting new epoch...\n",
            "Step 1501. Batch loss: 0.198665589094162 Batch accuracy: 0.9375\n",
            "Step 1601. Batch loss: 0.19792616367340088 Batch accuracy: 0.9296875\n",
            "Step 1701. Batch loss: 0.20804110169410706 Batch accuracy: 0.9375\n",
            "Step 1801. Batch loss: 0.13611319661140442 Batch accuracy: 0.9609375\n",
            "Starting new epoch...\n",
            "Step 1901. Batch loss: 0.13695989549160004 Batch accuracy: 0.953125\n",
            "Step 2001. Batch loss: 0.1301625519990921 Batch accuracy: 0.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGSRvdL5j6iz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ad02e7-7cf0-4810-a9d1-e1860ea70ef2"
      },
      "source": [
        "test_preds = tf.argmax(model1(data.test_data), axis=1,\n",
        "                       output_type=tf.int32)\n",
        "acc = tf.reduce_mean(tf.cast(tf.equal(test_preds, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(\"Test accuracy: {}\".format(acc))\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.9477999806404114\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Initialization=Uniform + Activation=Sigmoid"
      ],
      "metadata": {
        "id": "djsBF3Otth2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W3 = tf.Variable(tf.random.uniform(shape = [784, 128],\n",
        "    minval=-0.1, maxval=0.1, seed=None))\n",
        "b3 = tf.Variable(np.zeros(128, dtype=np.float32))\n",
        "\n",
        "W4 = tf.Variable(tf.random.uniform(shape = [128, 10],\n",
        "    minval=-0.1, maxval=0.1, seed=None))\n",
        "b4 = tf.Variable(np.zeros(10, dtype=np.float32))\n",
        "\n",
        "\n",
        "def model2(inputs):\n",
        "  #Computing the hidden layers - We choose relu activation function\n",
        "  h3 = tf.nn.sigmoid(tf.matmul(inputs,W3)+b3)\n",
        "  h4 = tf.nn.sigmoid(tf.matmul(h3,W4)+b4)\n",
        "\n",
        "\n",
        "\n",
        "  return h4\n",
        "\n"
      ],
      "metadata": {
        "id": "tVaFG8D2m4nD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "\n",
        "for step in range(train_steps+1):\n",
        "    image_batch, label_batch = data.next_batch()\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape2:\n",
        "        logits2 = model2(image_batch)\n",
        "        xent2 = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits2, labels=label_batch))\n",
        "  \n",
        "    grads3 = tape2.gradient(xent2, [W3, b3])\n",
        "    W3.assign_sub(learning_rate * grads3[0])\n",
        "    b3.assign_sub(learning_rate * grads3[1])\n",
        "\n",
        "    grads4 = tape2.gradient(xent2, [W4, b4])\n",
        "    W4.assign_sub(learning_rate * grads4[0])\n",
        "    b4.assign_sub(learning_rate * grads4[1])\n",
        "\n",
        "    \n",
        "    # every so often we print loss/accuracy\n",
        "    if not step % 100:\n",
        "        preds = tf.argmax(logits2, axis=1, output_type=tf.int32)\n",
        "        acc = tf.reduce_mean(tf.cast(tf.equal(preds, label_batch),\n",
        "                             tf.float32))\n",
        "        print(\"Step {}. Batch loss: {} Batch accuracy: {}\".format(step+1, xent2, acc))\n",
        "    \n",
        "del tape2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1D5Ynv7jnYeK",
        "outputId": "84601ee1-9bdf-47cf-f275-cfaed07b623f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1. Batch loss: 1.7207486629486084 Batch accuracy: 0.8671875\n",
            "Step 101. Batch loss: 1.718675971031189 Batch accuracy: 0.859375\n",
            "Step 201. Batch loss: 1.7365882396697998 Batch accuracy: 0.796875\n",
            "Starting new epoch...\n",
            "Step 301. Batch loss: 1.6872715950012207 Batch accuracy: 0.8671875\n",
            "Step 401. Batch loss: 1.695929765701294 Batch accuracy: 0.859375\n",
            "Step 501. Batch loss: 1.7340885400772095 Batch accuracy: 0.828125\n",
            "Step 601. Batch loss: 1.6672061681747437 Batch accuracy: 0.890625\n",
            "Starting new epoch...\n",
            "Step 701. Batch loss: 1.7002074718475342 Batch accuracy: 0.8515625\n",
            "Step 801. Batch loss: 1.6881155967712402 Batch accuracy: 0.875\n",
            "Step 901. Batch loss: 1.7110002040863037 Batch accuracy: 0.8828125\n",
            "Step 1001. Batch loss: 1.6974231004714966 Batch accuracy: 0.7890625\n",
            "Step 1101. Batch loss: 1.6838717460632324 Batch accuracy: 0.8359375\n",
            "Starting new epoch...\n",
            "Step 1201. Batch loss: 1.6746387481689453 Batch accuracy: 0.859375\n",
            "Step 1301. Batch loss: 1.7153035402297974 Batch accuracy: 0.8125\n",
            "Step 1401. Batch loss: 1.6880993843078613 Batch accuracy: 0.8359375\n",
            "Step 1501. Batch loss: 1.6510505676269531 Batch accuracy: 0.8515625\n",
            "Step 1601. Batch loss: 1.67783522605896 Batch accuracy: 0.8359375\n",
            "Starting new epoch...\n",
            "Step 1701. Batch loss: 1.6662230491638184 Batch accuracy: 0.8828125\n",
            "Step 1801. Batch loss: 1.655012607574463 Batch accuracy: 0.8984375\n",
            "Step 1901. Batch loss: 1.651261568069458 Batch accuracy: 0.8828125\n",
            "Step 2001. Batch loss: 1.6604868173599243 Batch accuracy: 0.8515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ae3e1c-c255-44c4-85b0-3940945970ed",
        "id": "zCxJ1zw9nysv"
      },
      "source": [
        "test_preds2 = tf.argmax(model2(data.test_data), axis=1,\n",
        "                       output_type=tf.int32)\n",
        "acc2 = tf.reduce_mean(tf.cast(tf.equal(test_preds2, data.test_labels),\n",
        "                             tf.float32))\n",
        "print(\"Test accuracy: {}\".format(acc2))\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 0.8744999766349792\n"
          ]
        }
      ]
    }
  ]
}