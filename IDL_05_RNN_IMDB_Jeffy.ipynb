{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffyelson/IntroductiontoDeepLearning/blob/main/IDL_05_RNN_IMDB_Jeffy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r-Chp8YyyvxN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZVE7aMvC4iXK"
      },
      "outputs": [],
      "source": [
        "# remove infrequent words. you can play with this parameter as it will likely impact model quality\n",
        "num_words = 20000\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c07u7Z7s4opk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a04c44f-c2b6-4b95-9a5e-9403702acc67"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# look at some sequences. words have been replaced with arbitrary index mappings\n",
        "# 1 is a special \"beginning of sequence\" marker\n",
        "# infrequent words have been replaced by the index 2\n",
        "# actual words start with index 4, 3 is never used (???)\n",
        "train_sequences[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AD6Elit34sTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5afeec67-70a7-4333-a8b6-38b35a60cde6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# labels are simply binary: sentiment can be positive or negative\n",
        "train_labels[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xHTMEyXW5KcQ"
      },
      "outputs": [],
      "source": [
        "# to restore words, load the word-to-index mapping\n",
        "word_to_index = tf.keras.datasets.imdb.get_word_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vv25lUc_5ckG"
      },
      "outputs": [],
      "source": [
        "# invert to get index-to-word mapping\n",
        "index_to_word = dict((index, word) for (word, index) in word_to_index.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CYX6F3AX5hpV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "d3d45d11-8f13-485f-bdf7-1ae592305d93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert UNKNOWN is an amazing actor and now the same being director UNKNOWN father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the UNKNOWN of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# we can convert a sequence to text by\n",
        "# - replacing each index by the respective word\n",
        "# - joining words together via spaces\n",
        "# note that we remove the beginning of sequence character and we have to subtract 3 from all indices\n",
        "# this is because, as mentioned above, the smallest indices are reserved for special characters\n",
        "# but for some reason this is not reflected in the mapping...\n",
        "\" \".join([index_to_word.get(index - 3, \"UNKNOWN\") for index in train_sequences[0][1:]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "9axnbnwR6q6W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "ba480f37-5d66-4f5d-b68c-ccb0a5f1522c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-282db15b0bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we cannot create a dataset :( this is because sequences are different length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# but tensors have to be \"rectangular\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    807\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \"\"\"\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m   4549\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4550\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4551\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4552\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4553\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    123\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 125\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    126\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1640\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    266\u001b[0m   \"\"\"\n\u001b[1;32m    267\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 268\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
          ]
        }
      ],
      "source": [
        "# we cannot create a dataset :( this is because sequences are different length\n",
        "# but tensors have to be \"rectangular\"\n",
        "train_data = tf.data.Dataset.from_tensor_slices(train_sequences, train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "m2lt9mE-9XO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb0cf04-c5aa-46c6-a3bf-6634ead93305"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2494"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# solution is padding all sequences to the maximum length.\n",
        "# first find the maximum length\n",
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "max_len = max(sequence_lengths)\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "677ZXcRu9nUe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "5d686732-f4cf-4354-b909-eaa6e48b8814"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# overview over sequence lengths in the data\n",
        "# could also look at mean, median, standard deviation...\n",
        "plt.hist(sequence_lengths, bins=80)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lYr10G5M9rWX"
      },
      "outputs": [],
      "source": [
        "# luckily there is a convenient function for padding\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pXEICggj-OL-"
      },
      "outputs": [],
      "source": [
        "# now we can create a dataset!\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "IPTPy5Ff-Q_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0b56fe-b997-4419-8085-8481e2f8cb56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2494)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# all sequences are... very long\n",
        "train_sequences_padded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ug0OSIGjf6ji"
      },
      "outputs": [],
      "source": [
        "# it would be better to do something like this\n",
        "# all sequences above maxlen will be truncated to that length\n",
        "# note: pad_sequences has \"pre\" and \"post\" options for both padding and truncation. one may be better than the other!\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=200)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n",
        "\n",
        "train_sequences_padded.shape\n",
        "train_data = train_data.shuffle(buffer_size=20000).batch(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "2ZW7YdDv_fRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50481f0d-e211-4e33-db1c-0bdf98a2bad2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(1, 'the'),\n",
              " (2, 'and'),\n",
              " (3, 'a'),\n",
              " (4, 'of'),\n",
              " (5, 'to'),\n",
              " (6, 'is'),\n",
              " (7, 'br'),\n",
              " (8, 'in'),\n",
              " (9, 'it'),\n",
              " (10, 'i'),\n",
              " (11, 'this'),\n",
              " (12, 'that'),\n",
              " (13, 'was'),\n",
              " (14, 'as'),\n",
              " (15, 'for'),\n",
              " (16, 'with'),\n",
              " (17, 'movie'),\n",
              " (18, 'but'),\n",
              " (19, 'film'),\n",
              " (20, 'on'),\n",
              " (21, 'not'),\n",
              " (22, 'you'),\n",
              " (23, 'are'),\n",
              " (24, 'his'),\n",
              " (25, 'have'),\n",
              " (26, 'he'),\n",
              " (27, 'be'),\n",
              " (28, 'one'),\n",
              " (29, 'all'),\n",
              " (30, 'at'),\n",
              " (31, 'by'),\n",
              " (32, 'an'),\n",
              " (33, 'they'),\n",
              " (34, 'who'),\n",
              " (35, 'so'),\n",
              " (36, 'from'),\n",
              " (37, 'like'),\n",
              " (38, 'her'),\n",
              " (39, 'or'),\n",
              " (40, 'just'),\n",
              " (41, 'about'),\n",
              " (42, \"it's\"),\n",
              " (43, 'out'),\n",
              " (44, 'has'),\n",
              " (45, 'if'),\n",
              " (46, 'some'),\n",
              " (47, 'there'),\n",
              " (48, 'what'),\n",
              " (49, 'good'),\n",
              " (50, 'more'),\n",
              " (51, 'when'),\n",
              " (52, 'very'),\n",
              " (53, 'up'),\n",
              " (54, 'no'),\n",
              " (55, 'time'),\n",
              " (56, 'she'),\n",
              " (57, 'even'),\n",
              " (58, 'my'),\n",
              " (59, 'would'),\n",
              " (60, 'which'),\n",
              " (61, 'only'),\n",
              " (62, 'story'),\n",
              " (63, 'really'),\n",
              " (64, 'see'),\n",
              " (65, 'their'),\n",
              " (66, 'had'),\n",
              " (67, 'can'),\n",
              " (68, 'were'),\n",
              " (69, 'me'),\n",
              " (70, 'well'),\n",
              " (71, 'than'),\n",
              " (72, 'we'),\n",
              " (73, 'much'),\n",
              " (74, 'been'),\n",
              " (75, 'bad'),\n",
              " (76, 'get'),\n",
              " (77, 'will'),\n",
              " (78, 'do'),\n",
              " (79, 'also'),\n",
              " (80, 'into'),\n",
              " (81, 'people'),\n",
              " (82, 'other'),\n",
              " (83, 'first'),\n",
              " (84, 'great'),\n",
              " (85, 'because'),\n",
              " (86, 'how'),\n",
              " (87, 'him'),\n",
              " (88, 'most'),\n",
              " (89, \"don't\"),\n",
              " (90, 'made'),\n",
              " (91, 'its'),\n",
              " (92, 'then'),\n",
              " (93, 'way'),\n",
              " (94, 'make'),\n",
              " (95, 'them'),\n",
              " (96, 'too'),\n",
              " (97, 'could'),\n",
              " (98, 'any'),\n",
              " (99, 'movies'),\n",
              " (100, 'after')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# for fun, you can look at the word-index mappings.\n",
        "# in this case, the mapping was done according to word frequency.\n",
        "# you can pass reverse=True to sorted() to look at the least common words.\n",
        "sorted(index_to_word.items())[:100]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences_padded[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb91RCusATfM",
        "outputId": "77d6f7d3-542d-4625-b8e8-1efc6863b534"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200,)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
        "loss_fn = tf.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "M9mqOYZJJz4f"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "u4fwUhqBACri"
      },
      "outputs": [],
      "source": [
        "# here is a high-level sketch for training RNNs\n",
        "U = tf.Variable(tf.random.uniform(shape = [100,100],minval=-0.1, maxval=0.1, seed=None))\n",
        "V = tf.Variable(tf.random.uniform(shape = [100,100],minval=-0.1, maxval=0.1, seed=None))\n",
        "W = tf.Variable(tf.random.uniform(shape = [100,100],minval=-0.1, maxval=0.1, seed=None))\n",
        "b = tf.Variable(np.zeros(20000, dtype=np.float32))\n",
        "c = tf.Variable(np.zeros(20000, dtype=np.float32))\n",
        "\n",
        "\n",
        "# training loop -- same thing as before!!\n",
        "# our data is now slightly different (each batch of sequences has a time axis, which is kinda new)\n",
        "# but all the related changes are hidden away at lower levels\n",
        "def train_loop():\n",
        "    for sequence_batch, label_batch in train_data:\n",
        "      train_step(sequence_batch, label_batch)\n",
        "\n",
        "\n",
        "\n",
        "# a single training step -- again, seems familiar?\n",
        "def train_step(sequences, labels):\n",
        "    #print(sequences[:100])\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = rnn_loop(sequences)\n",
        "        loss = loss_fn(labels, logits)\n",
        "\n",
        "\n",
        "\n",
        "    variables = [U,V,W,b,c]\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "      \n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
        "    acc = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
        "    print(\"Training Loss: {} Accuracy: {}\".format(loss, acc))\n",
        "        \n",
        "\n",
        "# here's where things start to change\n",
        "# we loop over the input time axis, and at each time step compute the new\n",
        "# hidden state based on the previous one as well as the current input\n",
        "# the state computation is hidden away in the rnn_step function and could be\n",
        "# arbitrarily complex.\n",
        "# in the general RNN, an output is computed at each time step, and the whole\n",
        "# sequence is returned. but in this case, since we only have one label for the\n",
        "# entire sequence, we only use the final state to compute one output and return it.\n",
        "# before the loop, the state need to be initialized somehow.\n",
        "\n",
        "\n",
        "def rnn_loop(sequences):\n",
        "  \n",
        "    old_state = tf.Variable(np.zeros((100,20000), dtype=np.float32))\n",
        "\n",
        "    for step in range(128):\n",
        "        x_t = sequences[:,step]\n",
        "        x_t = tf.one_hot(x_t, depth=num_words)\n",
        "\n",
        "        new_state = rnn_step(old_state, x_t)\n",
        "        \n",
        "\n",
        "        old_state = new_state\n",
        "\n",
        "    o_t = output_layer(new_state)\n",
        "    o_t = tf.keras.layers.Dense(1)(o_t)\n",
        "\n",
        "    return o_t\n",
        "\n",
        "\n",
        "# see formulas in the book ;)\n",
        "def rnn_step(state, x_t):\n",
        "  val1 = tf.matmul(U,x_t)\n",
        "  val2 = tf.matmul(W,state)\n",
        "  at = val1 + val2+b \n",
        "  ht = tf.nn.tanh(at)\n",
        "  return ht\n",
        "\n",
        "def output_layer(new_state):\n",
        "  out = c+tf.matmul(V,new_state)\n",
        "  return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loop()"
      ],
      "metadata": {
        "id": "OfuJM0KOVapg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caef3037-7ccd-42ae-cca1-b0fbadc6d692"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6929500699043274 Accuracy: 0.5199999809265137\n",
            "Training Loss: 0.7048528790473938 Accuracy: 0.5699999928474426\n",
            "Training Loss: 0.7198087573051453 Accuracy: 0.44999998807907104\n",
            "Training Loss: 0.9476001262664795 Accuracy: 0.5\n",
            "Training Loss: 1.090156078338623 Accuracy: 0.5199999809265137\n",
            "Training Loss: 0.8144937753677368 Accuracy: 0.6100000143051147\n",
            "Training Loss: 1.160149335861206 Accuracy: 0.5400000214576721\n",
            "Training Loss: 1.6205350160598755 Accuracy: 0.5099999904632568\n",
            "Training Loss: 1.8817635774612427 Accuracy: 0.5600000023841858\n",
            "Training Loss: 2.3287742137908936 Accuracy: 0.4000000059604645\n",
            "Training Loss: 0.922217845916748 Accuracy: 0.46000000834465027\n",
            "Training Loss: 3.155104398727417 Accuracy: 0.47999998927116394\n",
            "Training Loss: 4.519530773162842 Accuracy: 0.5600000023841858\n",
            "Training Loss: 0.7221064567565918 Accuracy: 0.5099999904632568\n",
            "Training Loss: 3.5134999752044678 Accuracy: 0.4300000071525574\n",
            "Training Loss: 1.7687726020812988 Accuracy: 0.4699999988079071\n",
            "Training Loss: 1.778297781944275 Accuracy: 0.4399999976158142\n",
            "Training Loss: 5.720318794250488 Accuracy: 0.5400000214576721\n",
            "Training Loss: 1.6904020309448242 Accuracy: 0.4699999988079071\n",
            "Training Loss: 2.9293601512908936 Accuracy: 0.47999998927116394\n",
            "Training Loss: 2.8201653957366943 Accuracy: 0.5\n",
            "Training Loss: 1.2688148021697998 Accuracy: 0.46000000834465027\n",
            "Training Loss: 1.389532446861267 Accuracy: 0.5\n",
            "Training Loss: 4.751722812652588 Accuracy: 0.38999998569488525\n",
            "Training Loss: 5.0835957527160645 Accuracy: 0.5199999809265137\n",
            "Training Loss: 3.7803211212158203 Accuracy: 0.5\n",
            "Training Loss: 1.0978165864944458 Accuracy: 0.5099999904632568\n",
            "Training Loss: 0.7156813740730286 Accuracy: 0.5299999713897705\n",
            "Training Loss: 0.7727181911468506 Accuracy: 0.4699999988079071\n",
            "Training Loss: 1.4184215068817139 Accuracy: 0.5199999809265137\n",
            "Training Loss: 6.202743053436279 Accuracy: 0.5699999928474426\n",
            "Training Loss: 1.2884278297424316 Accuracy: 0.5299999713897705\n",
            "Training Loss: 0.7570416331291199 Accuracy: 0.49000000953674316\n",
            "Training Loss: 2.7673022747039795 Accuracy: 0.5199999809265137\n",
            "Training Loss: 1.3393787145614624 Accuracy: 0.4000000059604645\n",
            "Training Loss: 2.0392563343048096 Accuracy: 0.49000000953674316\n",
            "Training Loss: 7.020686149597168 Accuracy: 0.5099999904632568\n",
            "Training Loss: 1.6894170045852661 Accuracy: 0.5400000214576721\n",
            "Training Loss: 0.9034065008163452 Accuracy: 0.5899999737739563\n",
            "Training Loss: 1.7298201322555542 Accuracy: 0.5\n",
            "Training Loss: 3.7021572589874268 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.3757688999176025 Accuracy: 0.49000000953674316\n",
            "Training Loss: 3.0941686630249023 Accuracy: 0.5199999809265137\n",
            "Training Loss: 0.7435360550880432 Accuracy: 0.3700000047683716\n",
            "Training Loss: 1.191234827041626 Accuracy: 0.38999998569488525\n",
            "Training Loss: 0.8700182437896729 Accuracy: 0.44999998807907104\n",
            "Training Loss: 1.4698095321655273 Accuracy: 0.5299999713897705\n",
            "Training Loss: 4.386186122894287 Accuracy: 0.5199999809265137\n",
            "Training Loss: 0.9628201127052307 Accuracy: 0.47999998927116394\n",
            "Training Loss: 2.317270278930664 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.05281400680542 Accuracy: 0.5099999904632568\n",
            "Training Loss: 2.94875431060791 Accuracy: 0.44999998807907104\n",
            "Training Loss: 0.8089298009872437 Accuracy: 0.4699999988079071\n",
            "Training Loss: 0.7448866963386536 Accuracy: 0.6100000143051147\n",
            "Training Loss: 1.9750311374664307 Accuracy: 0.5799999833106995\n",
            "Training Loss: 1.2469507455825806 Accuracy: 0.47999998927116394\n",
            "Training Loss: 1.5004897117614746 Accuracy: 0.44999998807907104\n",
            "Training Loss: 0.92035311460495 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.457027554512024 Accuracy: 0.4699999988079071\n",
            "Training Loss: 1.2550832033157349 Accuracy: 0.46000000834465027\n",
            "Training Loss: 1.316409707069397 Accuracy: 0.44999998807907104\n",
            "Training Loss: 1.1305007934570312 Accuracy: 0.5299999713897705\n",
            "Training Loss: 0.7268334031105042 Accuracy: 0.5199999809265137\n",
            "Training Loss: 0.950554370880127 Accuracy: 0.6399999856948853\n",
            "Training Loss: 2.6893417835235596 Accuracy: 0.46000000834465027\n",
            "Training Loss: 0.7028313279151917 Accuracy: 0.49000000953674316\n",
            "Training Loss: 2.0519907474517822 Accuracy: 0.44999998807907104\n",
            "Training Loss: 2.7108068466186523 Accuracy: 0.47999998927116394\n",
            "Training Loss: 0.692180871963501 Accuracy: 0.46000000834465027\n",
            "Training Loss: 1.1458854675292969 Accuracy: 0.3799999952316284\n",
            "Training Loss: 1.2804087400436401 Accuracy: 0.4000000059604645\n",
            "Training Loss: 2.669140338897705 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.2914750576019287 Accuracy: 0.47999998927116394\n",
            "Training Loss: 1.2820558547973633 Accuracy: 0.47999998927116394\n",
            "Training Loss: 0.7286142706871033 Accuracy: 0.5199999809265137\n",
            "Training Loss: 1.8650671243667603 Accuracy: 0.44999998807907104\n",
            "Training Loss: 1.510227918624878 Accuracy: 0.5199999809265137\n",
            "Training Loss: 1.7558882236480713 Accuracy: 0.49000000953674316\n",
            "Training Loss: 0.7383524179458618 Accuracy: 0.5699999928474426\n",
            "Training Loss: 0.7095949649810791 Accuracy: 0.5400000214576721\n",
            "Training Loss: 1.9588180780410767 Accuracy: 0.5400000214576721\n",
            "Training Loss: 3.596217155456543 Accuracy: 0.5\n",
            "Training Loss: 2.488816261291504 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.2893786430358887 Accuracy: 0.5899999737739563\n",
            "Training Loss: 1.039297103881836 Accuracy: 0.47999998927116394\n",
            "Training Loss: 0.7731172442436218 Accuracy: 0.5699999928474426\n",
            "Training Loss: 0.9097221493721008 Accuracy: 0.46000000834465027\n",
            "Training Loss: 0.8218158483505249 Accuracy: 0.47999998927116394\n",
            "Training Loss: 1.659051775932312 Accuracy: 0.4699999988079071\n",
            "Training Loss: 2.4839577674865723 Accuracy: 0.5799999833106995\n",
            "Training Loss: 1.310551643371582 Accuracy: 0.5099999904632568\n",
            "Training Loss: 3.0070552825927734 Accuracy: 0.550000011920929\n",
            "Training Loss: 3.237109422683716 Accuracy: 0.550000011920929\n",
            "Training Loss: 1.4935756921768188 Accuracy: 0.550000011920929\n",
            "Training Loss: 2.073439836502075 Accuracy: 0.5299999713897705\n",
            "Training Loss: 2.5419864654541016 Accuracy: 0.5199999809265137\n",
            "Training Loss: 1.540207028388977 Accuracy: 0.47999998927116394\n",
            "Training Loss: 1.1969726085662842 Accuracy: 0.5\n",
            "Training Loss: 2.0428411960601807 Accuracy: 0.550000011920929\n",
            "Training Loss: 2.970153570175171 Accuracy: 0.5699999928474426\n",
            "Training Loss: 0.8821020722389221 Accuracy: 0.5099999904632568\n",
            "Training Loss: 3.3001394271850586 Accuracy: 0.5600000023841858\n",
            "Training Loss: 1.4632259607315063 Accuracy: 0.44999998807907104\n",
            "Training Loss: 1.648303508758545 Accuracy: 0.5099999904632568\n",
            "Training Loss: 1.5221002101898193 Accuracy: 0.4399999976158142\n",
            "Training Loss: 2.536978244781494 Accuracy: 0.5299999713897705\n",
            "Training Loss: 1.7895982265472412 Accuracy: 0.5699999928474426\n",
            "Training Loss: 1.9321125745773315 Accuracy: 0.5400000214576721\n",
            "Training Loss: 1.1700645685195923 Accuracy: 0.4699999988079071\n",
            "Training Loss: 2.5853774547576904 Accuracy: 0.550000011920929\n",
            "Training Loss: 1.707463264465332 Accuracy: 0.46000000834465027\n",
            "Training Loss: 3.09413480758667 Accuracy: 0.41999998688697815\n",
            "Training Loss: 1.3251348733901978 Accuracy: 0.41999998688697815\n",
            "Training Loss: 3.3338732719421387 Accuracy: 0.5600000023841858\n",
            "Training Loss: 2.088670253753662 Accuracy: 0.44999998807907104\n",
            "Training Loss: 1.1220253705978394 Accuracy: 0.41999998688697815\n",
            "Training Loss: 1.3773475885391235 Accuracy: 0.550000011920929\n",
            "Training Loss: 2.0868074893951416 Accuracy: 0.46000000834465027\n",
            "Training Loss: 5.489879131317139 Accuracy: 0.47999998927116394\n",
            "Training Loss: 3.5449090003967285 Accuracy: 0.5799999833106995\n",
            "Training Loss: 1.4703271389007568 Accuracy: 0.5\n",
            "Training Loss: 1.1568318605422974 Accuracy: 0.38999998569488525\n",
            "Training Loss: 1.213630199432373 Accuracy: 0.5400000214576721\n",
            "Training Loss: 1.2544881105422974 Accuracy: 0.41999998688697815\n",
            "Training Loss: 6.561083793640137 Accuracy: 0.5400000214576721\n",
            "Training Loss: 1.0541150569915771 Accuracy: 0.5199999809265137\n",
            "Training Loss: 1.2292659282684326 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.017073154449463 Accuracy: 0.4699999988079071\n",
            "Training Loss: 1.4654269218444824 Accuracy: 0.4399999976158142\n",
            "Training Loss: 2.605278730392456 Accuracy: 0.5199999809265137\n",
            "Training Loss: 13.672440528869629 Accuracy: 0.5299999713897705\n",
            "Training Loss: 4.122354507446289 Accuracy: 0.5600000023841858\n",
            "Training Loss: 1.51627779006958 Accuracy: 0.550000011920929\n",
            "Training Loss: 2.7051303386688232 Accuracy: 0.5199999809265137\n",
            "Training Loss: 4.181233882904053 Accuracy: 0.4699999988079071\n",
            "Training Loss: 0.7920140027999878 Accuracy: 0.49000000953674316\n",
            "Training Loss: 2.3757543563842773 Accuracy: 0.5199999809265137\n",
            "Training Loss: 2.71242094039917 Accuracy: 0.550000011920929\n",
            "Training Loss: 4.499112129211426 Accuracy: 0.6000000238418579\n",
            "Training Loss: 5.6779584884643555 Accuracy: 0.44999998807907104\n",
            "Training Loss: 0.7053167223930359 Accuracy: 0.5400000214576721\n",
            "Training Loss: 0.8576313257217407 Accuracy: 0.4099999964237213\n",
            "Training Loss: 6.755767345428467 Accuracy: 0.5299999713897705\n",
            "Training Loss: 1.6417864561080933 Accuracy: 0.4399999976158142\n",
            "Training Loss: 2.0106983184814453 Accuracy: 0.5\n",
            "Training Loss: 2.170762062072754 Accuracy: 0.5299999713897705\n",
            "Training Loss: 1.3331676721572876 Accuracy: 0.4099999964237213\n",
            "Training Loss: 1.0116606950759888 Accuracy: 0.47999998927116394\n",
            "Training Loss: 0.9269844889640808 Accuracy: 0.5099999904632568\n",
            "Training Loss: 1.9188610315322876 Accuracy: 0.4699999988079071\n",
            "Training Loss: 0.9351257085800171 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.7076336145401 Accuracy: 0.5099999904632568\n",
            "Training Loss: 2.2199454307556152 Accuracy: 0.5400000214576721\n",
            "Training Loss: 2.4557268619537354 Accuracy: 0.4699999988079071\n",
            "Training Loss: 0.9399421811103821 Accuracy: 0.5299999713897705\n",
            "Training Loss: 0.7899794578552246 Accuracy: 0.47999998927116394\n",
            "Training Loss: 5.027094841003418 Accuracy: 0.4099999964237213\n",
            "Training Loss: 1.5490297079086304 Accuracy: 0.550000011920929\n",
            "Training Loss: 1.9102014303207397 Accuracy: 0.4699999988079071\n",
            "Training Loss: 2.1991851329803467 Accuracy: 0.550000011920929\n",
            "Training Loss: 2.635493278503418 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.4749773740768433 Accuracy: 0.5\n",
            "Training Loss: 0.7739033699035645 Accuracy: 0.5299999713897705\n",
            "Training Loss: 1.2507928609848022 Accuracy: 0.5600000023841858\n",
            "Training Loss: 1.0346814393997192 Accuracy: 0.49000000953674316\n",
            "Training Loss: 0.7190923094749451 Accuracy: 0.4300000071525574\n",
            "Training Loss: 1.0140210390090942 Accuracy: 0.41999998688697815\n",
            "Training Loss: 1.8183420896530151 Accuracy: 0.5600000023841858\n",
            "Training Loss: 0.7874683141708374 Accuracy: 0.41999998688697815\n",
            "Training Loss: 0.8016363382339478 Accuracy: 0.5\n",
            "Training Loss: 2.677016496658325 Accuracy: 0.550000011920929\n",
            "Training Loss: 2.6449697017669678 Accuracy: 0.5299999713897705\n",
            "Training Loss: 2.5783908367156982 Accuracy: 0.4699999988079071\n",
            "Training Loss: 2.619537353515625 Accuracy: 0.5299999713897705\n",
            "Training Loss: 2.49159836769104 Accuracy: 0.4699999988079071\n",
            "Training Loss: 0.8517403602600098 Accuracy: 0.44999998807907104\n",
            "Training Loss: 2.155874490737915 Accuracy: 0.6200000047683716\n",
            "Training Loss: 2.286745548248291 Accuracy: 0.5\n",
            "Training Loss: 2.20503306388855 Accuracy: 0.5199999809265137\n",
            "Training Loss: 1.494431734085083 Accuracy: 0.5199999809265137\n",
            "Training Loss: 3.044135332107544 Accuracy: 0.5799999833106995\n",
            "Training Loss: 1.5917952060699463 Accuracy: 0.5299999713897705\n",
            "Training Loss: 1.07216215133667 Accuracy: 0.44999998807907104\n",
            "Training Loss: 1.577175498008728 Accuracy: 0.4099999964237213\n",
            "Training Loss: 0.7701053023338318 Accuracy: 0.5400000214576721\n",
            "Training Loss: 0.7878774404525757 Accuracy: 0.49000000953674316\n",
            "Training Loss: 2.351365089416504 Accuracy: 0.5099999904632568\n",
            "Training Loss: 1.6398756504058838 Accuracy: 0.5199999809265137\n",
            "Training Loss: 0.8166854977607727 Accuracy: 0.41999998688697815\n",
            "Training Loss: 3.2600340843200684 Accuracy: 0.5699999928474426\n",
            "Training Loss: 2.8029065132141113 Accuracy: 0.4399999976158142\n",
            "Training Loss: 2.7680656909942627 Accuracy: 0.4399999976158142\n",
            "Training Loss: 2.2685914039611816 Accuracy: 0.4300000071525574\n",
            "Training Loss: 1.273508906364441 Accuracy: 0.5099999904632568\n",
            "Training Loss: 1.3087042570114136 Accuracy: 0.550000011920929\n",
            "Training Loss: 2.0971758365631104 Accuracy: 0.5699999928474426\n",
            "Training Loss: 1.1643985509872437 Accuracy: 0.47999998927116394\n",
            "Training Loss: 4.243040084838867 Accuracy: 0.5299999713897705\n",
            "Training Loss: 0.6818169951438904 Accuracy: 0.5199999809265137\n",
            "Training Loss: 4.478002071380615 Accuracy: 0.550000011920929\n",
            "Training Loss: 5.673565864562988 Accuracy: 0.4699999988079071\n",
            "Training Loss: 1.772971510887146 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.4373457431793213 Accuracy: 0.5099999904632568\n",
            "Training Loss: 2.116588830947876 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.0689126253128052 Accuracy: 0.5600000023841858\n",
            "Training Loss: 1.0160599946975708 Accuracy: 0.5\n",
            "Training Loss: 6.395806312561035 Accuracy: 0.5600000023841858\n",
            "Training Loss: 0.6928590536117554 Accuracy: 0.49000000953674316\n",
            "Training Loss: 1.7239418029785156 Accuracy: 0.46000000834465027\n",
            "Training Loss: 2.3488380908966064 Accuracy: 0.49000000953674316\n",
            "Training Loss: 2.0120110511779785 Accuracy: 0.47999998927116394\n",
            "Training Loss: 2.509594202041626 Accuracy: 0.49000000953674316\n",
            "Training Loss: 6.299848556518555 Accuracy: 0.5199999809265137\n",
            "Training Loss: 2.1262223720550537 Accuracy: 0.4000000059604645\n",
            "Training Loss: 2.3861403465270996 Accuracy: 0.5099999904632568\n",
            "Training Loss: 1.4094828367233276 Accuracy: 0.4399999976158142\n",
            "Training Loss: 4.219099044799805 Accuracy: 0.550000011920929\n",
            "Training Loss: 3.3480958938598633 Accuracy: 0.5199999809265137\n",
            "Training Loss: 3.182208776473999 Accuracy: 0.5799999833106995\n",
            "Training Loss: 4.026271820068359 Accuracy: 0.4699999988079071\n",
            "Training Loss: 2.279029369354248 Accuracy: 0.4699999988079071\n",
            "Training Loss: 1.196707844734192 Accuracy: 0.47999998927116394\n",
            "Training Loss: 4.869105815887451 Accuracy: 0.41999998688697815\n",
            "Training Loss: 2.0849785804748535 Accuracy: 0.550000011920929\n",
            "Training Loss: 1.5876777172088623 Accuracy: 0.47999998927116394\n",
            "Training Loss: 3.4730770587921143 Accuracy: 0.5\n",
            "Training Loss: 1.0930086374282837 Accuracy: 0.46000000834465027\n",
            "Training Loss: 2.0295915603637695 Accuracy: 0.5600000023841858\n",
            "Training Loss: 5.1573486328125 Accuracy: 0.5400000214576721\n",
            "Training Loss: 4.693293571472168 Accuracy: 0.49000000953674316\n",
            "Training Loss: 2.342996597290039 Accuracy: 0.47999998927116394\n",
            "Training Loss: 2.65128231048584 Accuracy: 0.5699999928474426\n",
            "Training Loss: 0.8327723145484924 Accuracy: 0.550000011920929\n",
            "Training Loss: 3.174819231033325 Accuracy: 0.44999998807907104\n",
            "Training Loss: 1.413385033607483 Accuracy: 0.5299999713897705\n",
            "Training Loss: 1.1301460266113281 Accuracy: 0.46000000834465027\n",
            "Training Loss: 3.4934661388397217 Accuracy: 0.46000000834465027\n",
            "Training Loss: 1.5734976530075073 Accuracy: 0.5099999904632568\n",
            "Training Loss: 1.6049165725708008 Accuracy: 0.550000011920929\n",
            "Training Loss: 3.737863063812256 Accuracy: 0.4300000071525574\n",
            "Training Loss: 2.032261610031128 Accuracy: 0.5099999904632568\n",
            "Training Loss: 2.4406685829162598 Accuracy: 0.550000011920929\n",
            "Training Loss: 1.7719992399215698 Accuracy: 0.47999998927116394\n",
            "Training Loss: 2.7036266326904297 Accuracy: 0.5799999833106995\n",
            "Training Loss: 1.3729069232940674 Accuracy: 0.5299999713897705\n",
            "Training Loss: 1.2238093614578247 Accuracy: 0.46000000834465027\n",
            "Training Loss: 0.9237333536148071 Accuracy: 0.5600000023841858\n",
            "Training Loss: 1.5480772256851196 Accuracy: 0.4099999964237213\n",
            "Training Loss: 2.7355661392211914 Accuracy: 0.6200000047683716\n",
            "Training Loss: 0.9799773693084717 Accuracy: 0.4300000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=max_len)"
      ],
      "metadata": {
        "id": "u7Hfy8YqFCgT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences_padded.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbPk7TMsF5SJ",
        "outputId": "60fb9256-31eb-4180-d42a-b98dda09f8e7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2494)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_padded, test_labels))\n"
      ],
      "metadata": {
        "id": "q1h9NtrjFOvN"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_data.shuffle(buffer_size=20000).batch(100)"
      ],
      "metadata": {
        "id": "c1Paip4hFlpH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop():\n",
        "  for sequence_batch,labels in test_data:\n",
        "      out=rnn_loop(sequence_batch)\n",
        "      loss = loss_fn(labels, out)\n",
        "      preds = tf.argmax(out, axis=1, output_type=tf.int64)\n",
        "  acc = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))\n",
        "  print(\"Test Loss: {} Test Accuracy: {}\".format(loss, acc))"
      ],
      "metadata": {
        "id": "q8Dm_vGFG5wI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_iQ7iQsLpNT",
        "outputId": "16a99476-07f0-4748-c59b-864e3d01ce89"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.6932953596115112 Test Accuracy: 0.5400000214576721\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}